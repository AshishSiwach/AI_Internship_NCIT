{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AshishSiwach/AI_Internship_NCIT/blob/main/Fine%20Tuning%20Code%20Notebooks/Individual%20Model%20Notebooks/Gemma_2b_QLORA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjoLmRu2IiSW"
      },
      "outputs": [],
      "source": [
        "#!pip install -q -U bitsandbytes transformers peft accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import os\n",
        "import sys\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from peft import PeftModel, PeftConfig\n",
        "from huggingface_hub import notebook_login\n",
        "from google.colab import userdata # Import the userdata module for Colab secrets\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "j5ACVzYcI6Et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "f2_ZeFc-TLHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelConfig:\n",
        "    \"\"\"\n",
        "    Configuration class for the CausalLM fine-tuning script.\n",
        "    \"\"\"\n",
        "    # --- Model Selection ---\n",
        "    # --- Model Selection ---\n",
        "    MODEL_NAME = \"google/gemma-2b-it\"\n",
        "\n",
        "\n",
        "    # --- Hugging Face Hub Integration ---\n",
        "    PUSH_TO_HUB = True\n",
        "    # IMPORTANT: Update this to your full Hugging Face repository ID (username/model-name)\n",
        "    HUB_MODEL_ID = \"TripleH/football-rebuttal-model-gemma-2b-qlora-final\"\n",
        "\n",
        "    # --- Data and Paths ---\n",
        "    # The input file path is now provided as a command-line argument to main.py\n",
        "    INPUT_DATA_PATH = \"/content/drive/MyDrive/Colab/football_dataset_contrarian_claims_with_reasoning.csv\"\n",
        "    OUTPUT_DIR = \"/content/drive/MyDrive/football_model/results\"\n",
        "    LOGGING_DIR = \"/content/drive/MyDrive/football_model/logs\"\n",
        "\n",
        "    # --- Preprocessing ---\n",
        "    INPUT_COLUMN = \"claim\"\n",
        "    TARGET_FOOTBALL_COLUMN = \"Football_Term\"\n",
        "    TARGET_REASONING_COLUMN = \"Reasoning\"\n",
        "\n",
        "    # --- Tokenization ---\n",
        "    MAX_SEQ_LENGTH = 384\n",
        "\n",
        "    # --- Training & Early Stopping ---\n",
        "    NUM_TRAIN_EPOCHS = 15\n",
        "    PER_DEVICE_TRAIN_BATCH_SIZE = 1\n",
        "    PER_DEVICE_EVAL_BATCH_SIZE = 1\n",
        "    GRADIENT_ACCUMULATION_STEPS = 4\n",
        "    WARMUP_STEPS = 100\n",
        "    WEIGHT_DECAY = 0.01\n",
        "    LEARNING_RATE = 2e-4\n",
        "\n",
        "    # Evaluate and save at the end of each epoch\n",
        "    EVALUATION_STRATEGY = \"epoch\"\n",
        "    SAVE_STRATEGY = \"epoch\"\n",
        "    LOGGING_STRATEGY = \"epoch\"\n",
        "\n",
        "    # Early stopping will halt training if validation loss doesn't improve for 3 evaluations\n",
        "    EARLY_STOPPING_PATIENCE = 3\n"
      ],
      "metadata": {
        "id": "S6DxNMFX4OTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import sys\n",
        "from datasets import Dataset\n",
        "\n",
        "\n",
        "def create_chat_prompt(example, config: ModelConfig):\n",
        "    \"\"\"\n",
        "    Creates a chat-formatted prompt with a system message to assign a role.\n",
        "    \"\"\"\n",
        "     # For Gemma, the system-like instruction is part of the first user message.\n",
        "    instruction = \"You are a witty football commentator who refutes claims with a clever, direct analogy. You MUST first provide a short, punchy 'Football Term' you would hear on a broadcast, followed by a 'Reasoning' that explains the connection. Your response must strictly follow this two-part format.\"\n",
        "    claim = f\"Refute this claim: '{example[config.INPUT_COLUMN]}'\"\n",
        "\n",
        "    # Combine the instruction and the specific task into a single user prompt.\n",
        "    user_prompt = f\"{instruction}\\n\\n{claim}\"\n",
        "\n",
        "    model_answer = f\"**Football Term:** {example[config.TARGET_FOOTBALL_COLUMN]}\\n**Reasoning:** {example[config.TARGET_REASONING_COLUMN]}\"\n",
        "\n",
        "    # The message list for Gemma should only contain 'user' and 'assistant' roles.\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": user_prompt},\n",
        "        {\"role\": \"assistant\", \"content\": model_answer}\n",
        "    ]\n",
        "\n",
        "    return {\"messages\": messages}\n",
        "\n",
        "\n",
        "def load_and_preprocess_data(config: ModelConfig, file_path: str) -> Dataset:\n",
        "    \"\"\"\n",
        "    Loads and preprocesses the data for CausalLM fine-tuning.\n",
        "    Accepts a file_path argument passed from the main script.\n",
        "    \"\"\"\n",
        "    print(f\"Loading data from {config.INPUT_FILE_PATH}...\")\n",
        "    try:\n",
        "        df = pd.read_csv(file_path, encoding='latin1')\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Dataset not found at '{config.INPUT_FILE_PATH}'\", file=sys.stderr)\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading CSV: {e}\", file=sys.stderr)\n",
        "        return None\n",
        "\n",
        "    df.columns = df.columns.str.strip()\n",
        "    dataset = Dataset.from_pandas(df)\n",
        "    dataset = dataset.map(lambda x: create_chat_prompt(x, config), remove_columns=dataset.column_names)\n",
        "    print(\"Data loading and formatting complete.\")\n",
        "    return dataset\n"
      ],
      "metadata": {
        "id": "N2Ag94WiI5Il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import warnings\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "# --- End Suppress Warnings ---\n",
        "\n",
        "import torch\n",
        "import sys\n",
        "from getpass import getpass\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "def run_training(config: ModelConfig, file_path: str):\n",
        "    \"\"\"Main function to orchestrate the fine-tuning process.\"\"\"\n",
        "    # --- Device and Auth ---\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"--- Using device: {device} ---\")\n",
        "\n",
        "    # --- Tokenizer and Data Loading ---\n",
        "    tokenizer = AutoTokenizer.from_pretrained(config.MODEL_NAME)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    def tokenize_function(examples):\n",
        "        formatted_text = tokenizer.apply_chat_template(examples['messages'], tokenize=False, add_generation_prompt=False)\n",
        "        return tokenizer(formatted_text, truncation=True, max_length=config.MAX_SEQ_LENGTH, padding=\"max_length\")\n",
        "\n",
        "    full_dataset = load_and_preprocess_data(config, file_path)\n",
        "    if full_dataset is None:\n",
        "        return\n",
        "\n",
        "    tokenized_dataset = full_dataset.map(tokenize_function, remove_columns=[\"messages\"])\n",
        "\n",
        "    train_test_split = tokenized_dataset.train_test_split(test_size=0.1)\n",
        "    train_dataset = train_test_split['train']\n",
        "    eval_dataset = train_test_split['test']\n",
        "\n",
        "    # --- QLoRA Configuration ---\n",
        "    bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.bfloat16, bnb_4bit_use_double_quant=True)\n",
        "\n",
        "    lora_config = LoraConfig(\n",
        "        r=16,\n",
        "        lora_alpha=32,\n",
        "        target_modules=['qkv_proj', 'o_proj', 'gate_up_proj', 'down_proj'],\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=\"CAUSAL_LM\",\n",
        "    )\n",
        "\n",
        "    # --- Model Loading and Preparation ---\n",
        "    model = AutoModelForCausalLM.from_pretrained(config.MODEL_NAME, quantization_config=bnb_config, device_map=\"auto\")\n",
        "    model = prepare_model_for_kbit_training(model)\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    model.config.use_cache = False\n",
        "\n",
        "    # --- Model Training ---\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=config.OUTPUT_DIR,\n",
        "        num_train_epochs=config.NUM_TRAIN_EPOCHS,\n",
        "        learning_rate=config.LEARNING_RATE,\n",
        "        per_device_train_batch_size=config.PER_DEVICE_TRAIN_BATCH_SIZE,\n",
        "        per_device_eval_batch_size=config.PER_DEVICE_EVAL_BATCH_SIZE,\n",
        "        gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,\n",
        "        warmup_steps=config.WARMUP_STEPS,\n",
        "        weight_decay=config.WEIGHT_DECAY,\n",
        "        eval_strategy=\"steps\",\n",
        "        save_strategy=\"steps\",\n",
        "        logging_strategy = \"steps\",\n",
        "        logging_dir=config.LOGGING_DIR,\n",
        "        load_best_model_at_end=True, # This is crucial for early stopping\n",
        "        metric_for_best_model=\"loss\", # Monitor validation loss\n",
        "        push_to_hub=config.PUSH_TO_HUB,\n",
        "        hub_model_id=config.HUB_MODEL_ID,\n",
        "        report_to=\"none\"\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset,\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=config.EARLY_STOPPING_PATIENCE)]\n",
        "    )\n",
        "\n",
        "    print(\"--- Starting QLoRA Fine-Tuning ---\")\n",
        "    trainer.train()\n",
        "    print(\"--- Fine-Tuning Complete ---\")\n",
        "\n",
        "    if config.PUSH_TO_HUB:\n",
        "        print(f\"Uploading LoRA adapters to Hugging Face Hub: {config.HUB_MODEL_ID}\")\n",
        "        trainer.push_to_hub()\n",
        "        print(\"Adapters uploaded successfully!\")\n"
      ],
      "metadata": {
        "id": "SpQShiH2JEmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "    # Initialize the configuration\n",
        "    config = ModelConfig()\n",
        "\n",
        "    # Pass the configuration and the command-line filename to the training utility\n",
        "    run_training(config, config.INPUT_FILE_PATH)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "6Gkeq_SbVsen"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "N4itbBH-O9IY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import PeftModel, PeftConfig\n",
        "import sys\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "class BatchInferenceConfig:\n",
        "    \"\"\"\n",
        "    Configuration class for the QLoRA inference script.\n",
        "    \"\"\"\n",
        "    # --- Model Selection ---\n",
        "    # The Hugging Face Hub repository ID of your fine-tuned LoRA adapters.\n",
        "    ADAPTER_MODEL_PATH = \"TripleH/football-rebuttal-model-gemma-2b-qlora-final\"\n",
        "\n",
        "    # --- Data Paths ---\n",
        "    # The path to your original dataset file.\n",
        "    INPUT_DATA_PATH = \"/content/drive/MyDrive/Colab/football_dataset_contrarian_claims_with_reasoning.csv\"\n",
        "    # The name of the file where the full predictions and ground truth will be saved.\n",
        "    OUTPUT_DATA_PATH = \"/content/drive/MyDrive/Colab/predictions_gemma-2b-it.csv\"\n",
        "    # The name for a cleaner CSV with just the claims and predictions.\n",
        "    CLEAN_OUTPUT_DATA_PATH = \"/content/drive/MyDrive/Colab/predictions_clean_gemma-2b-it.csv\"\n",
        "\n",
        "    # --- Generation Parameters ---\n",
        "    # Switching to sampling with temperature for more creative outputs.\n",
        "    MAX_NEW_TOKENS = 128\n",
        "    DO_SAMPLE = True\n",
        "    TEMPERATURE = 0.1  # Controls randomness. Higher is more creative.\n",
        "    TOP_P = 0.9        # Nucleus sampling. Helps prevent the model from going off-topic.\n",
        "\n",
        "# --- END CONFIGURATION ---"
      ],
      "metadata": {
        "id": "9i2IYQu-O8s4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_and_tokenizer(adapter_path: str):\n",
        "    \"\"\"\n",
        "    Loads the base model in 4-bit, applies the LoRA adapters,\n",
        "    and prepares it for inference.\n",
        "    \"\"\"\n",
        "    print(f\"Loading fine-tuned model from Hub: {adapter_path}...\")\n",
        "    try:\n",
        "        # First, get the base model name from the adapter's config using PeftConfig\n",
        "        peft_config = PeftConfig.from_pretrained(adapter_path)\n",
        "        base_model_name = peft_config.base_model_name_or_path\n",
        "\n",
        "        print(f\"Base model identified as: {base_model_name}\")\n",
        "\n",
        "        # Configure 4-bit quantization to load the base model efficiently\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        )\n",
        "\n",
        "        # Load the base model with quantization\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            base_model_name,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\", # Automatically map layers to GPU/CPU\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        # Load the tokenizer\n",
        "        tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        # Load the LoRA adapter and merge it into the base model\n",
        "        print(\"Loading LoRA adapters and merging with base model...\")\n",
        "        model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "        model = model.merge_and_unload() # Merge adapters for faster inference\n",
        "\n",
        "        model.eval() # Set the model to evaluation mode\n",
        "        print(\"Model and tokenizer loaded successfully.\")\n",
        "        return model, tokenizer\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\", file=sys.stderr)\n",
        "        return None, None\n"
      ],
      "metadata": {
        "id": "E-8pm7iCKZMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_prediction(model, tokenizer, claim_text: str, config: BatchInferenceConfig):\n",
        "    \"\"\"Generates a prediction for a single claim text using the chat template.\"\"\"\n",
        "\n",
        "    # Define the system and user prompts, exactly matching the training format.\n",
        "    instruction = \"You are a witty football commentator who refutes claims with a clever, direct analogy. You MUST first provide a short, punchy 'Football Term' you would hear on a broadcast, followed by a 'Reasoning' that explains the connection. Your response must strictly follow this two-part format.\"\n",
        "    claim = f\"Refute this claim: '{claim_text}'\"\n",
        "\n",
        "    # Combine the instruction and the specific task into a single user prompt.\n",
        "    user_prompt = f\"{instruction}\\n\\n{claim}\"\n",
        "\n",
        "    # The message list for Gemma should only contain 'user' and 'assistant' roles.\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "    ]\n",
        "\n",
        "    # Apply the template and get the input IDs.\n",
        "    # add_generation_prompt=True adds the special tokens to signal the model to start generating.\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device)\n",
        "\n",
        "    # Generate output using the fine-tuned model\n",
        "    with torch.no_grad():\n",
        "        output_sequences = model.generate(\n",
        "            input_ids=inputs,\n",
        "            max_new_tokens=config.MAX_NEW_TOKENS,\n",
        "            do_sample=config.DO_SAMPLE,\n",
        "            temperature=config.TEMPERATURE,\n",
        "            top_p=config.TOP_P,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            use_cache = False\n",
        "        )\n",
        "\n",
        "    # Decode the generated tokens back to a string\n",
        "    # We only want the newly generated part, so we slice the output\n",
        "    generated_text = tokenizer.decode(output_sequences[0][inputs.shape[1]:], skip_special_tokens=True)\n",
        "    return generated_text.strip()\n"
      ],
      "metadata": {
        "id": "pt_DQJhSWlXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run batch inference on the entire dataset.\n",
        "    \"\"\"\n",
        "    config = BatchInferenceConfig()\n",
        "\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"--- Using device: {device} ---\")\n",
        "\n",
        "    model, tokenizer = load_model_and_tokenizer(config.ADAPTER_MODEL_PATH)\n",
        "    if model is None:\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        df = pd.read_csv(config.INPUT_DATA_PATH, encoding='latin1')\n",
        "        df.columns = df.columns.str.strip()\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Input data file not found at '{config.INPUT_DATA_PATH}'.\", file=sys.stderr)\n",
        "        return\n",
        "\n",
        "    predictions = []\n",
        "    ground_truths = []\n",
        "\n",
        "    print(f\"\\n--- Generating predictions for {len(df)} claims ---\")\n",
        "    # Use tqdm for a progress bar\n",
        "    for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=\"Processing claims\"):\n",
        "        claim = row['claim']\n",
        "\n",
        "        # Generate the model's prediction\n",
        "        prediction = generate_prediction(model, tokenizer, claim, config)\n",
        "        predictions.append(prediction)\n",
        "\n",
        "        # Construct the ground truth for comparison\n",
        "        ground_truth = f\"**Football Term:** {row['Football_Term']}\\n**Reasoning:** {row['Reasoning']}\"\n",
        "        ground_truths.append(ground_truth)\n",
        "\n",
        "    # Add the new columns to the DataFrame\n",
        "    df['ground_truth'] = ground_truths\n",
        "    df['model_prediction'] = predictions\n",
        "\n",
        "    # Save the full results to the first CSV file\n",
        "    df.to_csv(config.OUTPUT_DATA_PATH, index=False)\n",
        "\n",
        "    # Create and save the second, cleaner CSV file\n",
        "    clean_df = df[['claim', 'model_prediction']]\n",
        "    clean_df.to_csv(config.CLEAN_OUTPUT_DATA_PATH, index=False)\n",
        "\n",
        "    print(\"\\n--- Batch Inference Complete ---\")\n",
        "    print(f\"Full results with ground truth saved to '{config.OUTPUT_DATA_PATH}'\")\n",
        "    print(f\"Clean predictions saved to '{config.CLEAN_OUTPUT_DATA_PATH}'\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "gwXGVjD7PR-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hyLWUffyPXHA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}